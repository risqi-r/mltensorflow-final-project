{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Deep Learning Model with GRU","metadata":{}},{"cell_type":"markdown","source":"## Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nsns.set_style(\"whitegrid\")\nwarnings.filterwarnings('ignore')\n\nfrom string import digits\nimport regex as re\nimport re\nimport nltk\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob, Word\nfrom collections import Counter\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import load_model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-27T11:58:11.003007Z","iopub.execute_input":"2022-07-27T11:58:11.003449Z","iopub.status.idle":"2022-07-27T11:58:24.379558Z","shell.execute_reply.started":"2022-07-27T11:58:11.003355Z","shell.execute_reply":"2022-07-27T11:58:24.378506Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Import Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/disaster-tweets/tweets.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.382151Z","iopub.execute_input":"2022-07-27T11:58:24.382820Z","iopub.status.idle":"2022-07-27T11:58:24.478595Z","shell.execute_reply.started":"2022-07-27T11:58:24.382772Z","shell.execute_reply":"2022-07-27T11:58:24.477685Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Statistical Check","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.480083Z","iopub.execute_input":"2022-07-27T11:58:24.480995Z","iopub.status.idle":"2022-07-27T11:58:24.515408Z","shell.execute_reply.started":"2022-07-27T11:58:24.480953Z","shell.execute_reply":"2022-07-27T11:58:24.514278Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Since target is a categorical numeric variable $int64$, we want to convert it to $str$ for better display of statistics","metadata":{}},{"cell_type":"code","source":"df.target = df.target.astype('str')\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.519913Z","iopub.execute_input":"2022-07-27T11:58:24.520270Z","iopub.status.idle":"2022-07-27T11:58:24.553289Z","shell.execute_reply.started":"2022-07-27T11:58:24.520239Z","shell.execute_reply":"2022-07-27T11:58:24.552112Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.describe(include = 'object')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.554820Z","iopub.execute_input":"2022-07-27T11:58:24.556065Z","iopub.status.idle":"2022-07-27T11:58:24.598488Z","shell.execute_reply.started":"2022-07-27T11:58:24.556016Z","shell.execute_reply":"2022-07-27T11:58:24.597597Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.describe(exclude = 'object')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.600149Z","iopub.execute_input":"2022-07-27T11:58:24.600513Z","iopub.status.idle":"2022-07-27T11:58:24.619047Z","shell.execute_reply.started":"2022-07-27T11:58:24.600482Z","shell.execute_reply":"2022-07-27T11:58:24.618036Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"- Total 11370 data\n- Most of the target variable is not talking about disaster\n- ID column contain unique values only. We need to drop it\n- We want to remove the keyword and location fields too so we can use only the essential features","metadata":{}},{"cell_type":"code","source":"train = df.drop(['id', 'keyword', 'location'], axis = 1)\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.620093Z","iopub.execute_input":"2022-07-27T11:58:24.620696Z","iopub.status.idle":"2022-07-27T11:58:24.637836Z","shell.execute_reply.started":"2022-07-27T11:58:24.620659Z","shell.execute_reply":"2022-07-27T11:58:24.636679Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Duplicated Text","metadata":{}},{"cell_type":"code","source":"print(\"Is there any duplicated text? \", train.duplicated(subset = ['text'], keep = False).any())\nprint(\"\")\nprint(\"Total duplicated text = \", train.duplicated(subset = ['text'], keep = False).sum())","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.639042Z","iopub.execute_input":"2022-07-27T11:58:24.640026Z","iopub.status.idle":"2022-07-27T11:58:24.668167Z","shell.execute_reply.started":"2022-07-27T11:58:24.639990Z","shell.execute_reply":"2022-07-27T11:58:24.666896Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We can see that our dataset has a total of 292 duplicate texts. We want to remove those duplicate values for a better dataset","metadata":{}},{"cell_type":"code","source":"train.drop_duplicates(subset = ['text'], keep = 'last', inplace = True)\ntrain.duplicated(subset = ['text'], keep = False).any()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.669546Z","iopub.execute_input":"2022-07-27T11:58:24.669956Z","iopub.status.idle":"2022-07-27T11:58:24.694779Z","shell.execute_reply.started":"2022-07-27T11:58:24.669916Z","shell.execute_reply":"2022-07-27T11:58:24.693857Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.698343Z","iopub.execute_input":"2022-07-27T11:58:24.699463Z","iopub.status.idle":"2022-07-27T11:58:24.710895Z","shell.execute_reply.started":"2022-07-27T11:58:24.699411Z","shell.execute_reply":"2022-07-27T11:58:24.709924Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now let's head into a quick EDA\n\n# EDA\n\nFor this EDA, we're using wordcloud to find most frequent word","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\ntexts = \" \".join(train.text)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.713260Z","iopub.execute_input":"2022-07-27T11:58:24.713805Z","iopub.status.idle":"2022-07-27T11:58:24.763561Z","shell.execute_reply.started":"2022-07-27T11:58:24.713736Z","shell.execute_reply":"2022-07-27T11:58:24.762629Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"wordclouds = WordCloud(collocations = False, \n                       background_color = 'white',\n                      max_font_size = 70,\n                      max_words = 250).generate(texts)\n\nplt.rcParams[\"figure.figsize\"] = (20, 10)\nplt.imshow(wordclouds)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:24.765275Z","iopub.execute_input":"2022-07-27T11:58:24.766009Z","iopub.status.idle":"2022-07-27T11:58:25.968728Z","shell.execute_reply.started":"2022-07-27T11:58:24.765962Z","shell.execute_reply":"2022-07-27T11:58:25.967833Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\n## Lowering","metadata":{}},{"cell_type":"code","source":"stopword = set(stopwords.words('english'))\nlemma = WordNetLemmatizer()\nrx = re.compile(r'([^\\W\\d_])\\1{2,}')\n\ndef pos_tagger(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:         \n        return None\n\ndef lemmatizing(text):\n    pos_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n    lemmatized_stc = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            lemmatized_stc.append(word)\n        else:       \n            lemmatized_stc.append(lemma.lemmatize(word, tag))\n    \n    return ' '.join(lemmatized_stc)\n\ndef clean(text):\n    text =  text.lower()\n    text_at = text.split()\n    text =  ' '.join([i for i in text_at if 'htt' not in i])\n    text = text.translate(str.maketrans('', '', digits))\n    text = re.sub(r'[^\\w]', ' ', text)\n    text = ' '.join(word for word in text.split() if word not in stopword)\n    text = lemmatizing(text)\n    text = re.sub(r'[^\\W\\d_]+', lambda x: Word(rx.sub(r'\\1\\1', x.group())).correct() if rx.search(x.group()) else x.group(), text)\n    return text\n\ntrain['clean'] = train.text.apply(lambda x: clean(x))\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:25.969813Z","iopub.execute_input":"2022-07-27T11:58:25.970149Z","iopub.status.idle":"2022-07-27T11:58:56.666722Z","shell.execute_reply.started":"2022-07-27T11:58:25.970119Z","shell.execute_reply":"2022-07-27T11:58:56.665513Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Counter","metadata":{}},{"cell_type":"code","source":"def counter_word(text_col):\n    count = Counter()\n    for text in text_col.values:\n        for word in text.split():\n            count[word] += 1\n    return count\n\ncounter = counter_word(train.clean)\n\nnum_unique_words = len(counter)\nnum_unique_words","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:56.668192Z","iopub.execute_input":"2022-07-27T11:58:56.668648Z","iopub.status.idle":"2022-07-27T11:58:56.733035Z","shell.execute_reply.started":"2022-07-27T11:58:56.668607Z","shell.execute_reply":"2022-07-27T11:58:56.731828Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = {'label':train.target.values, 'text':train.clean.values}\ndfs = pd.DataFrame(data = ds)\n\n# Split dataset into training and validation set\ntrain_size = int(dfs.shape[0] * 0.8)\n\ntrain_df = dfs[:train_size]\nval_df = dfs[train_size:]\n\n# split text and labels\ntrain_sentences = train_df.text.to_numpy()\ntrain_labels = train_df.label.to_numpy().astype(int)\nval_sentences = val_df.text.to_numpy()\nval_labels = val_df.label.to_numpy().astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:56.735623Z","iopub.execute_input":"2022-07-27T11:58:56.736685Z","iopub.status.idle":"2022-07-27T11:58:56.747882Z","shell.execute_reply.started":"2022-07-27T11:58:56.736578Z","shell.execute_reply":"2022-07-27T11:58:56.746474Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# vectorize a text corpus by turning each text into a sequence of integers\ntokenizer = Tokenizer(num_words=num_unique_words)\ntokenizer.fit_on_texts(train_sentences) # fit only to training","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:56.748996Z","iopub.execute_input":"2022-07-27T11:58:56.749320Z","iopub.status.idle":"2022-07-27T11:58:56.942254Z","shell.execute_reply.started":"2022-07-27T11:58:56.749291Z","shell.execute_reply":"2022-07-27T11:58:56.941022Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# each word has unique index\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\nval_sequences = tokenizer.texts_to_sequences(val_sentences)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:56.943597Z","iopub.execute_input":"2022-07-27T11:58:56.943941Z","iopub.status.idle":"2022-07-27T11:58:57.117463Z","shell.execute_reply.started":"2022-07-27T11:58:56.943907Z","shell.execute_reply":"2022-07-27T11:58:57.116200Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Pad the sequences to have the same length\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Max number of words in a sequence\nmax_length = 20\n\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\nval_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\ntrain_padded.shape, val_padded.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-27T11:58:57.119083Z","iopub.execute_input":"2022-07-27T11:58:57.119523Z","iopub.status.idle":"2022-07-27T11:58:57.181950Z","shell.execute_reply.started":"2022-07-27T11:58:57.119478Z","shell.execute_reply":"2022-07-27T11:58:57.180685Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T13:03:02.846163Z","iopub.execute_input":"2022-07-27T13:03:02.846582Z","iopub.status.idle":"2022-07-27T13:03:02.880378Z","shell.execute_reply.started":"2022-07-27T13:03:02.846550Z","shell.execute_reply":"2022-07-27T13:03:02.879158Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\n\nmodel = keras.models.Sequential()\nmodel.add(layers.Embedding(num_unique_words, 300, input_length=max_length))\nmodel.add(layers.Bidirectional(\n                                tf.keras.layers.GRU(128,\n                                return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(layers.Dense(128, activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(64, activation=\"relu\"))\nmodel.add(layers.Dense(1, activation=\"sigmoid\"))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T13:03:03.189739Z","iopub.execute_input":"2022-07-27T13:03:03.190507Z","iopub.status.idle":"2022-07-27T13:03:03.689215Z","shell.execute_reply.started":"2022-07-27T13:03:03.190468Z","shell.execute_reply":"2022-07-27T13:03:03.688052Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# compile model\nlr = 0.000003\n\nmodel.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n                optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n                metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T13:03:03.691839Z","iopub.execute_input":"2022-07-27T13:03:03.692524Z","iopub.status.idle":"2022-07-27T13:03:03.705717Z","shell.execute_reply.started":"2022-07-27T13:03:03.692475Z","shell.execute_reply":"2022-07-27T13:03:03.704255Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"callbacks = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\nmodel_trained = model.fit(train_padded,\n                          train_labels,\n                          epochs=30,\n                          callbacks=callbacks,\n                          validation_data=(val_padded, val_labels))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T13:03:03.721962Z","iopub.execute_input":"2022-07-27T13:03:03.722726Z","iopub.status.idle":"2022-07-27T13:15:17.642315Z","shell.execute_reply.started":"2022-07-27T13:03:03.722679Z","shell.execute_reply":"2022-07-27T13:15:17.640862Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.plot(model_trained.history[\"accuracy\"], label=\"train accuracy\")\nplt.plot(model_trained.history[\"val_accuracy\"], label=\"validation accuracy\")\nplt.plot(model_trained.history[\"loss\"], label=\"train loss\")\nplt.plot(model_trained.history[\"val_loss\"], label=\"validation loss\")\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T13:15:17.644233Z","iopub.execute_input":"2022-07-27T13:15:17.644581Z","iopub.status.idle":"2022-07-27T13:15:17.914293Z","shell.execute_reply.started":"2022-07-27T13:15:17.644548Z","shell.execute_reply":"2022-07-27T13:15:17.913042Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model.evaluate(val_padded, val_labels)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T13:15:17.915508Z","iopub.execute_input":"2022-07-27T13:15:17.915864Z","iopub.status.idle":"2022-07-27T13:15:19.337231Z","shell.execute_reply.started":"2022-07-27T13:15:17.915832Z","shell.execute_reply":"2022-07-27T13:15:19.336245Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}